{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Satellite Image Processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mprofinder/Colab-Notebooks/blob/master/Satellite_Image_Processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rwIadju7G3cL"
      },
      "source": [
        "# Satellite Image Processing with Deep Learning\n",
        "## Dr. Tristan Behrens\n",
        "\n",
        "In this notebook we will:\n",
        "- Solve the EuroSAT-10 classification problem,\n",
        "- to that end use Convolutional Neural Networks,\n",
        "- decrease overfitting with dropout,\n",
        "- decrease overfitting even further with data augmentation, and\n",
        "- solve the problem with transfer learning.\n",
        "\n",
        "## Miscellaneous\n",
        "\n",
        "- Subscribe to my YouTube channel: https://www.youtube.com/channel/UCcMEBxcDM034JyJ8J3cggRg\n",
        "- Add me on LinkedIn: https://www.linkedin.com/in/dr-tristan-behrens-ai-guru-734967a2/\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TQ16LaARG3cN"
      },
      "source": [
        "## Make sure that we have TensorFlow 2 enabled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cC1bLcDvG3cP",
        "colab": {}
      },
      "source": [
        "%tensorflow_version 2.x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Px5ywWDNG3cd"
      },
      "source": [
        "## Import all necessary modules  and check TensorFlow version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zh5KlVNrG3cg",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith(\"2.\"), \"You have TensorFlow version {}, 2.X is required, please upgrade.\".format(tf.__version__)\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras import models, layers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "uY4tv0a4ah0J"
      },
      "source": [
        "## Set some parameters and prepare for training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0Aau8P5Gah-M",
        "colab": {}
      },
      "source": [
        "histories = {}\n",
        "epochs = 100\n",
        "batch_size = 1024"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4GExuqFsG3co"
      },
      "source": [
        "## Load and split EuroSAT-dataset.\n",
        "\n",
        "We split the data into three subsets:\n",
        "- Train: For training the Neural Network.\n",
        "- Validate: To see how good the Neural Network is after each epoch.\n",
        "- Test: To see how good the Neural Network is after training.\n",
        "\n",
        "Link: [EuroSAT](https://github.com/phelber/eurosat)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nYdibo15G3cp",
        "colab": {}
      },
      "source": [
        "(dataset_train_original, dataset_validate_original, dataset_test_original), info = tfds.load(\n",
        "    name=\"eurosat/rgb\", \n",
        "    split=[\"train[:70%]\", \"train[70%:90%]\", \"train[90%:]\"],\n",
        "    with_info=True,\n",
        "    as_supervised=True\n",
        ")\n",
        "print(info)\n",
        "print(\"Train:   \", len(list(dataset_train_original)))\n",
        "print(\"Validate:\", len(list(dataset_validate_original)))\n",
        "print(\"Test:    \", len(list(dataset_test_original)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vtZoUJNtG3cw"
      },
      "source": [
        "## Look at your data!\n",
        "\n",
        "As always: Never trust the source of your data. Even if you created it. Do not worry, this is not paranoia. It is just a good way how to ensure the quality of your project. Always look at your data, because most of the times if there is something not so nice, the data is the cause."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iAwNcMGvNV6X",
        "colab": {}
      },
      "source": [
        "class_names = [\"annual crop\", \"forest\", \"herbaceous vegetation\", \"highway\", \"industrial\", \"pasture\", \"permanent crop\", \"residential\", \"river\", \"sea & lake\"]\n",
        "\n",
        "def label_to_string(label):\n",
        "  return class_names[label]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LChp7pOjG3cy",
        "colab": {}
      },
      "source": [
        "index = 1\n",
        "plt.figure(figsize=(20, 2))\n",
        "for dataset_example in dataset_train_original.take(6):\n",
        "    image, label = dataset_example\n",
        "\n",
        "    plt.subplot(1, 6, index)\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(\"Label: {} {}\".format(label.numpy(), label_to_string(label.numpy())))\n",
        "    index += 1\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8PNGuLhrG3c5"
      },
      "source": [
        "## Preparing the datasets with tf.data.\n",
        "\n",
        "We will make sure that all images are normalized and that all labes are one-hot-encoded.\n",
        "\n",
        "Link: [tf.data: Build TensorFlow input pipelines](https://www.tensorflow.org/guide/data)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "-rjd0ZcDG3c7",
        "colab": {}
      },
      "source": [
        "def encode(image, label):\n",
        "    image_encoded = tf.image.convert_image_dtype(image, dtype=tf.float32)\n",
        "    label_encoded = tf.one_hot(label, depth=10)\n",
        "    return image_encoded, label_encoded\n",
        "\n",
        "dataset_train = dataset_train_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_validate = dataset_validate_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_test = dataset_test_original.map(lambda image, label: encode(image, label)).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "D8GKidgGG3dD"
      },
      "source": [
        "## A second look at our data.\n",
        "\n",
        "This is how the data looks like that the Neural Network will be trained on."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "R9RPxyyIG3dG",
        "colab": {}
      },
      "source": [
        "index = 1\n",
        "plt.figure(figsize=(20, 2))\n",
        "for dataset_example in dataset_train.take(6):\n",
        "    image, label = dataset_example\n",
        "\n",
        "    plt.subplot(1, 6, index)\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(\"Label:\\n {}\".format(label.numpy()))\n",
        "    index += 1\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LcvN5iQ2G3dL"
      },
      "source": [
        "## Create a Deep Neural Network to solve our classification problem - Convolutional Neural Network.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "oiA62ErpG3dL",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "# Convolutional block 1.\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(64, 64, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Convolutional block 2.\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "\n",
        "# Convolutional block 3.\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "\n",
        "# Latent space.\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "# Classifier.\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Igm2pw-G3dR"
      },
      "source": [
        "---\n",
        "\n",
        "The architecture exhibits three structures:\n",
        "\n",
        "1. The Convolutional blocks act as Feature Extractors.\n",
        "2. The Flatten layer facilitates a Latent Space.\n",
        "3. The Dense layers is the actual classifier."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "IKsmb4szG3dT"
      },
      "source": [
        "## Attach optimizer, loss, and metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cMUwx-wKG3dU",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OfjuGdxTG3dh"
      },
      "source": [
        "## How good is our ANN before training?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "trqPkFSvG3di",
        "colab": {}
      },
      "source": [
        "loss, acc = model.evaluate(dataset_test.batch(32), verbose=0)\n",
        "print(\"Loss: {}\".format(loss))\n",
        "print(\"Accuracy: {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RaMMcUDRG3do"
      },
      "source": [
        "## ANN training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LruQUszWG3dp",
        "colab": {}
      },
      "source": [
        "history = model.fit(\n",
        "    dataset_train.shuffle(10000).batch(batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=dataset_validate.batch(batch_size)\n",
        ")\n",
        "\n",
        "histories[\"Baseline\"] = history"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Lrv3V0AWG3dt"
      },
      "source": [
        "## Inspect the history.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "afcWFJZHG3dv",
        "colab": {}
      },
      "source": [
        "def plot_history(history):\n",
        "  plt.figure(figsize=(10, 4))\n",
        "\n",
        "  plt.subplot(1, 2, 1)\n",
        "  plt.plot(history.history[\"loss\"], label=\"loss\")\n",
        "  plt.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Losses\")\n",
        "\n",
        "  plt.subplot(1, 2, 2)\n",
        "  plt.plot(history.history[\"accuracy\"], label=\"accuracy\")\n",
        "  plt.plot(history.history[\"val_accuracy\"], label=\"val_accuracy\")\n",
        "  plt.legend()\n",
        "  plt.title(\"Metrics\")\n",
        "\n",
        "  plt.show()\n",
        "  plt.close()\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_Lpuei4adHE-"
      },
      "source": [
        "---\n",
        "\n",
        "Although the problem is solved, we have some severe overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "SsIYy321G3d0"
      },
      "source": [
        "## How good is our ANN after training?\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v9tVgnvVG3d7",
        "colab": {}
      },
      "source": [
        "loss, acc = model.evaluate(dataset_test.batch(32), verbose=0)\n",
        "print(\"Loss: {}\".format(loss))\n",
        "print(\"Accuracy: {}\".format(acc))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "yUdumMX202M6"
      },
      "source": [
        "## Reducing overfitting by adding dropout.\n",
        "\n",
        "Dropout enforces generalization by randomly dropping activations during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "t5DbZiOM6rPV",
        "colab": {}
      },
      "source": [
        "dataset_train = dataset_train_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_validate = dataset_validate_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_test = dataset_test_original.map(lambda image, label: encode(image, label)).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EOe6oOOH6tb6",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(64, 64, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "XrSpE4Vj6uvj",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train.shuffle(10000).batch(batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=dataset_validate.batch(batch_size)\n",
        ")\n",
        "\n",
        "histories[\"Dropout\"] = history\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cppWnp9SBgnE"
      },
      "source": [
        "## Reducing overfitting with Dropout and Data Augmentation.\n",
        "\n",
        "Data Augmentation is artificially extending the dataset by changing the original data randomly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nG5Agfnw6TUD",
        "colab": {}
      },
      "source": [
        "def augment(image, label):\n",
        "    image_augmented = image\n",
        "    image_augmented = tf.image.random_flip_left_right(image_augmented)\n",
        "    image_augmented = tf.image.random_flip_up_down(image_augmented)\n",
        "    image_augmented = tf.image.random_contrast(image_augmented, 0.5, 1.0)\n",
        "    image_augmented = tf.image.random_brightness(image_augmented, 0.25)\n",
        "    image_augmented = tf.image.random_hue(image_augmented, 0.2)\n",
        "    return image_augmented, label\n",
        "\n",
        "dataset_train = dataset_train_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_train = dataset_train.map(lambda image, label: augment(image, label))\n",
        "dataset_validate = dataset_validate_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_test = dataset_test_original.map(lambda image, label: encode(image, label)).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EGLZ1k1ufhpx"
      },
      "source": [
        "--- \n",
        "\n",
        "Let us have a look at how the augmented data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YUxJF-Fp8kPk",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(20, 2))\n",
        "index = 1\n",
        "\n",
        "dataset_example = list(dataset_train_original.take(1))[0]\n",
        "image, label = dataset_example\n",
        "plt.subplot(1, 6, index)\n",
        "plt.imshow(image.numpy())\n",
        "plt.title(\"Original\")\n",
        "index += 1\n",
        "\n",
        "for _ in range(6):\n",
        "    dataset_example = list(dataset_train.take(1))[0]\n",
        "    image, label = dataset_example\n",
        "    plt.subplot(1, 7, index)\n",
        "    plt.imshow(image.numpy())\n",
        "    plt.title(\"Augmented\")\n",
        "    index += 1\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fTHVeq90-0ek",
        "colab": {}
      },
      "source": [
        "model = models.Sequential()\n",
        "\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", input_shape=(64, 64, 3)))\n",
        "model.add(layers.Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.2))\n",
        "\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(64, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.MaxPooling2D((2, 2)))\n",
        "model.add(layers.Dropout(0.3))\n",
        "\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Conv2D(128, (3, 3), activation=\"relu\", padding=\"same\"))\n",
        "model.add(layers.Dropout(0.4))\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dense(128, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3QeYmHKP-9Os",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train.shuffle(10000).batch(batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=dataset_validate.batch(batch_size)\n",
        ")\n",
        "\n",
        "histories[\"Dropout-Augmentation\"] = history\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AxvueT1OVdwP",
        "colab_type": "text"
      },
      "source": [
        "## Transfer Learning."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_vbxwDErZZW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset_train = dataset_train_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_train = dataset_train.map(lambda image, label: augment(image, label))\n",
        "dataset_validate = dataset_validate_original.map(lambda image, label: encode(image, label)).cache()\n",
        "dataset_test = dataset_test_original.map(lambda image, label: encode(image, label)).cache()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2mXXu-TLRpqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras import applications\n",
        "\n",
        "pretrained_model = applications.VGG19(\n",
        "    weights=\"imagenet\", \n",
        "    include_top=False, \n",
        "    input_shape=(64, 64, 3)\n",
        ")\n",
        "\n",
        "#for layer in pretrained_model.layers[:-5]:\n",
        "#  layer.trainable = False\n",
        "\n",
        "pretrained_model.summary()\n",
        "\n",
        "pretrained_model.trainable = False\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "model.add(pretrained_model)\n",
        "\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "model.add(layers.Dropout(0.25))\n",
        "model.add(layers.Dense(1024, activation=\"relu\"))\n",
        "model.add(layers.Dropout(0.5))\n",
        "model.add(layers.Dense(10, activation=\"softmax\"))\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTR4IAzaSj_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(\n",
        "    optimizer=\"adam\",\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "\n",
        "history = model.fit(\n",
        "    dataset_train.shuffle(10000).batch(batch_size),\n",
        "    epochs=epochs,\n",
        "    validation_data=dataset_validate.batch(batch_size)\n",
        ")\n",
        "\n",
        "histories[\"TransferLearning-Dropout-Augmentation\"] = history\n",
        "\n",
        "plot_history(history)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bSzqQ3ZoE3OT"
      },
      "source": [
        "## Compare results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tD0NrfLEE3Ya",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(10, 4))\n",
        "  \n",
        "plt.subplot(1, 2, 1)\n",
        "for title, history in histories.items():\n",
        "  plt.plot(history.history[\"val_loss\"], label=title)\n",
        "plt.legend()\n",
        "plt.title(\"Validation Loss\")\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "for title, history in histories.items():\n",
        "  plt.plot(history.history[\"val_accuracy\"], label=title)\n",
        "plt.legend()\n",
        "plt.title(\"Validation Accuracy\")\n",
        "\n",
        "plt.show()\n",
        "plt.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "5gyY4qHYG3eI"
      },
      "source": [
        "# Summary.\n",
        "\n",
        "For solving image processing problems, Convolutional Neural Networks are state of the art. There are several architecures available. We focused on interleaving Convolutional layers with Pooling layers.\n",
        "\n",
        "As with all other use cases, overfitting can be a problem. We looked at Dropout and Data Augmentation for compensating overfitting."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cuFIoxyfhSVR",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}